# -*- coding: utf-8 -*-
"""X-Ray abnormalities detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-sDW96zDamUl71xNalY_31NU3XkcV5Gz

# Imports
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/My\ Drive/Colab\ Notebooks/MURA-v1.1.zip

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For exampale, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from skimage.io import imread
from skimage.color import rgb2gray, gray2rgb
from skimage.transform import resize
from skimage.exposure import equalize_adapthist, adjust_gamma
import cv2
from tqdm import tqdm
import time
import gc
import shutil
from glob import glob

from sklearn.metrics import cohen_kappa_score

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable

import matplotlib.pyplot as plt

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

"""# Data loading and preprocessing

Load image paths.
"""

train_image_paths = pd.read_csv('/content/MURA-v1.1/train_image_paths.csv')

"""Rename the column containing the path."""

train_image_paths.columns = ['path']

"""Extract labels from folder names and correct all paths in our DataFrame."""

labels = torch.tensor(train_image_paths['path'].str.contains('positive').astype('uint8').values).long()

train_image_paths['wrist'] = train_image_paths['path'].str.contains('WRIST').astype('uint8').values
train_image_paths['shoulder'] = train_image_paths['path'].str.contains('SHOULDER').astype('uint8').values
train_image_paths['hand'] = train_image_paths['path'].str.contains('HAND').astype('uint8').values
train_image_paths['elbow'] = train_image_paths['path'].str.contains('ELBOW').astype('uint8').values
train_image_paths['finger'] = train_image_paths['path'].str.contains('FINGER').astype('uint8').values
train_image_paths['forearm'] = train_image_paths['path'].str.contains('FOREARM').astype('uint8').values
train_image_paths['humerus'] = train_image_paths['path'].str.contains('HUMERUS').astype('uint8').values

train_image_paths['path'] = train_image_paths['path'].apply(lambda x: '/content/' + x)

train_image_paths

len(train_image_paths), len(labels)

def unison_shuffled_copies(a, b):
    assert len(a) == len(b)
    p = np.random.permutation(len(a))
    return a.iloc[p], b[p]

train_image_paths, labels = unison_shuffled_copies(train_image_paths, labels)

train_image_paths.head()

OUT_DIM = (320, 320)

"""Split dataset to chunks of size Â±1000."""

label_chunks = np.array_split(labels, 70)
image_path_chunks = np.array_split(train_image_paths['path'].values, 70)

data_chunks = [np.array(image_path_chunks), label_chunks]

print('Amount of true labels: {}, false labels: {}'.format(len(labels[labels == 1]), len(labels[labels == 0])))

import albumentations as alb

aug_amount = 3

aug = alb.Compose([
    alb.ShiftScaleRotate(rotate_limit=20, scale_limit=(-0.02, 0.02), p=1, border_mode=cv2.BORDER_CONSTANT),
#     alb.RandomContrast(p=0.5)
])

def preprocess(image_paths, do_aug=True):
    if do_aug:
        images = torch.zeros(len(image_paths) * (aug_amount + 1), 3, OUT_DIM[0], OUT_DIM[1])
    
        for i, image_path in enumerate(tqdm(image_paths, position=0, leave=True)):
            image = gray2rgb(resize(imread(image_path), OUT_DIM, mode='constant'))
            for j in range(aug_amount):
                aug_image = aug(image=image)['image']
                aug_image = np.swapaxes(aug_image, -1, 0)
                images[i, :, :, :] = torch.tensor(aug_image)
                i += 1
            image = np.swapaxes(image, -1, 0)
            images[i, :, :, :] = torch.tensor(image)
    else:
        images = torch.zeros(len(image_paths), 3, OUT_DIM[0], OUT_DIM[1])
    
        for i, image_path in enumerate(tqdm(image_paths, position=0, leave=True)):
            image = gray2rgb(resize(imread(image_path), OUT_DIM, mode='constant'))
            image = np.swapaxes(image, -1, 0)
            images[i, :, :, :] = torch.tensor(image)
        
    return images

"""# Model

I am using a pretrained DenseNet model. It has already learnt a set of features, so we can save up time training all the layers ourselves.
"""

!export CUDA_LAUNCH_BLOCKING=1

from torchvision import models

densenet169 = models.densenet169(pretrained=True)
densenet169.classifier = nn.Linear(in_features=1664, out_features=2, bias=True)
densenet169.cuda()

"""As we have a simple binary classification, we can use weighted Cross entropy loss function. The optimizer is Adam."""

A = len(labels[labels == 1])
N = len(labels[labels == 0])

W1 = N / (A + N)
W0 = A / (A + N)

learning_rate = 0.0001

criterion = nn.CrossEntropyLoss(weight=torch.tensor([W0, W1]).cuda())
optimizer = torch.optim.Adam(densenet169.parameters(), lr=learning_rate)

"""Training part was borrowed from [here (clickable)](https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py)"""

def train(train_loader, model, criterion, optimizer, epoch):
    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    # switch to train mode
    model.train()

    end = time.time()
    for i, (images, target) in enumerate(train_loader):
        # measure data loading time
        data_time.update(time.time() - end)
        images = normalize(images, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

        target = target.long().cuda(async=True)
        input_var = Variable(images).cuda()
        target_var = Variable(target)

        # compute output
        output = model(input_var)
        pred = np.argmax(output.data.cpu(), axis=1)

        loss = criterion(output, target_var)

        # measure accuracy and record loss
        f1 = f1_score(pred, target_var.cpu())

        losses.update(loss.data, images.size(0))
        top1.update(f1, images.size(0))        

        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        if i % 10 == 0:
            print('Epoch: [{0}][{1}/{2}]\t'
                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  'Kappa {f1_score:.4f}\t'.format(
                   epoch, i, len(train_loader), batch_time=batch_time,
                   data_time=data_time, loss=losses, f1_score=f1))


def validate(val_loader, model, criterion):
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()

    # switch to evaluate mode
    model.eval()

    end = time.time()
    for i, (images, target) in enumerate(val_loader):
        target = target.cuda(async=True)
        images = normalize(images, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        target = target.long().cuda(async=True)
        with torch.no_grad():
            input_var = torch.autograd.Variable(images).cuda()
            target_var = torch.autograd.Variable(target).cuda()

            # compute output
            output = model(input_var)
            pred = np.argmax(output.data.cpu(), axis=1)

            loss = criterion(output, target_var)

            # measure accuracy and record loss
            f1 = f1_score(pred, target_var.cpu())

            losses.update(loss.data, images.size(0))
            top1.update(kappa_score, images.size(0))

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

        if i % 10 == 0:
            print('Test: [{0}/{1}]\t'
                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  'Kappa {f1_score:.4f}\t'.format(
                   i, len(val_loader), batch_time=batch_time,
                   loss=losses, f1_score=f1))

    return top1.avg


def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):
    torch.save(state, filename)
    if is_best:
        shutil.copyfile(filename, 'model_best.pth.tar')


class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def adjust_learning_rate(optimizer, epoch):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    lr = learning_rate * (0.1 ** (epoch // 30))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr


def normalize(tensor, mean, std):
    for i in range(3):
        if len(tensor.shape) == 4:
            tensor[:, i, :, :] = (tensor[:, i, :, :] - mean[i]) / std[i]
        else:
            tensor[i, :, :] = (tensor[i, :, :] - mean[i]) / std[i]
        
    return tensor

from sklearn.model_selection import train_test_split

batch_size = 8
best_prec1 = 0.0

for image_paths, labels in zip(data_chunks[0][:5], data_chunks[1][:5]):
    images = preprocess(image_paths)
    labels = np.repeat(labels, aug_amount + 1) # due to the augmentation

    X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.25, random_state=17)
    
    train_ds = TensorDataset(X_train, y_train)
    val_ds = TensorDataset(X_val, y_val)
    
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)
    for epoch in range(5):
        adjust_learning_rate(optimizer, epoch)

        # train for one epoch
        train(train_loader, densenet169, criterion, optimizer, epoch)

        # evaluate on validation set
        prec1 = validate(val_loader, densenet169, criterion)

        # remember best prec1 and save checkpoint
        is_best = prec1 > best_prec1
        best_prec1 = max(prec1, best_prec1)
        
        save_checkpoint({
            'epoch': epoch + 1,
            'arch': 'densenet169',
            'state_dict': densenet169.state_dict(),
            'best_prec1': best_prec1,
        }, is_best)

pretrained_dict = torch.load('/content/model_best.pth.tar')['state_dict']
densenet169_dict = densenet169.state_dict()

# 1. filter out unnecessary keys
pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in densenet169_dict}
# 2. overwrite entries in the existing state dict
densenet169_dict.update(pretrained_dict)
# 3. load the new state dict
densenet169.load_state_dict(pretrained_dict)

test_folders = glob('/content/MURA-v1.1/valid/*/*/*')
test_folders[:3]

class PatientCase:
    def __init__(self, image_paths, label):
        self.image_paths = image_paths
        self.label = label
        
    def get_image_paths():
        return self.image_paths

all_cases = []

for test_folder in test_folders:
    files = glob(test_folder + '/*')
    label = 1  if 'positive' in test_folder.split('_') else 0
    all_cases.append(PatientCase(files, label))

total_pred = []
labels = []

for case in all_cases:
    with torch.no_grad():
        test_images = preprocess(case.image_paths, do_aug=False)
        test_images = normalize(test_images, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        labels.append(case.label)
        pred = densenet169(Variable(test_images).cuda())
        total_pred.append(pred.cpu().numpy())

total_pred[90:100]

res = []

for pred in total_pred:
  res.append(np.argmax(np.sum(pred, axis=0) / len(pred)))

# res

f1_score = f1_score(res, labels)

f1_score

